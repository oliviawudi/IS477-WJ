
# Data Lifecycle

## 1. Plan

- Defined our core research questions:

  - Did the strength of the relationship between financial literacy and household credit card debt change after COVID-19?
  - Did households with higher financial literacy show smaller increases in debt after COVID-19?

- Chose two complementary data sources:

  - **SHED 2020 & 2024** microdata for household-level financial behavior.
  - **CFPB Consumer Credit Trends (credit cards – borrower risk profiles)** for macro-level credit market conditions.

- Identified key concepts and variables:

  - Financial resilience: EF1 (emergency savings), EF2 (3-month coverage), EF3_a–h (emergency responses).
  - Credit card behavior: C2A (card ownership), C4A (unpaid balance frequency).
  - Credit risk groups from CFPB (Deep subprime → Super-prime).

- Planned to build a **reproducible workflow** using Python notebooks, small Python scripts, and a Snakemake pipeline to connect all steps from raw data to final figures.

---

## 2. Collect / Acquire

We obtained data directly from the original federal sources:

- **SHED 2020 and 2024**
  - Source: Board of Governors of the Federal Reserve System.  
  - Format: SAS files (`public2020.sas7bdat`, `shed2024.sas7bdat`) plus PDF codebooks.
  - Action:
    - Downloaded the official ZIP files from the Federal Reserve website.
    - Extracted the SAS files.
    - Converted them to CSV using `pyreadstat` in a Jupyter notebook.

- **CFPB Consumer Credit Trends – Credit Cards (Borrower Risk Profiles)**
  - Source: Consumer Financial Protection Bureau.
  - Format: CSV (`volume_data_Score_Level_CRC.csv`).
  - Action:
    - Downloaded the raw CSV from the CFPB open data portal.
    - Saved it as the raw CFPB dataset for later aggregation and integration.

URLs for all datasets are listed in the report and in `Reproduce.md`.

---

## 3. Store & Organize

We separated **code**, **metadata**, and **data**:

- **GitHub repository (`IS477-WJ`)**
  - Stores all code and documentation:
    - Jupyter notebooks: `IS_477_Data_Cleaning.ipynb`, `IS_477_Analysis.ipynb`, `workflow_and_provenance.ipynb`.
    - Python scripts used by Snakemake (in `scripts/`).
    - `Snakefile` defining the workflow.
    - Project documentation: `README.md`, `ProjectPlan.md`, `StatusReport.md`, `Reproduce.md`.
    - `requirements.txt` for software dependencies.
  - Large data files (SAS, full CSVs, PNGs) are **excluded** from git via `.gitignore`.

- **Box storage**
  - Holds all large artifacts:
    - Raw SHED SAS files and raw CFPB CSV.
    - Cleaned CSVs (e.g., `shed20_cleaned2.csv`, `shed24_cleaned2.csv`, `cfpd_cleaned2.csv`, `shed_year.csv`, `crc_year.csv`, `merged.csv`).
    - All final plots and tables.
  - Shared Box link is included in the report so others at UIUC can access the full output.

The folder structure is documented in `README.md` and `Reproduce.md`, so users know where each type of file lives.

---

## 4. Integrate & Enrich

We enriched and combined the datasets in several stages:

1. **Derive a proxy credit-risk measure in SHED**
   - Computed a numeric `risk_score` for each respondent based on:
     - Lack of emergency savings (EF1), inability to cover 3 months (EF2),
     - Risky emergency responses (EF3_B, EF3_D, EF3_E),
     - No credit card access (C2A),
     - Frequency of unpaid balances (C4A).
   - Mapped `risk_score` into categorical **`proxy_credit_group`**:
     - Super-prime, Prime, Near-prime, Subprime, Deep subprime.

2. **Subsample to align years**
   - SHED 2024 has more respondents than 2020.
   - Drew a random subsample of 2024 respondents so the 2024 sample size matches 2020.
   - This prevents year-to-year comparisons from being dominated by the larger 2024 sample.

3. **Aggregate SHED**
   - Filtered to respondents who own credit cards when analyzing card debt.
   - Aggregated to year × `proxy_credit_group` level (e.g., emergency savings rates, stress indicators).

4. **Aggregate CFPB**
   - Restricted CFPB data to credit cards only.
   - Summed monthly `vol`/`vol_unadj` into **annual totals** for 2020 and 2024, grouped by `credit_score_group`.
   - Created a transformed variable `log_vol` for easier comparison across credit score groups.

5. **Merge SHED and CFPB**
   - Aligned SHED `proxy_credit_group` with CFPB `credit_score_group`.
   - Merged the two datasets on **year × credit score group** to produce `merged.csv`.
   - This final dataset combines household-level behavior indicators with national credit volume for each risk tier.

These integration steps are implemented as Snakemake rules (`shed_group`, `cfpb_group`, `merge`) and corresponding Python scripts.

---

## 5. Quality & Clean

Data cleaning and quality checks were performed before analysis:

- **SHED 2024**
  - Selected EF and C variables related to financial resilience and credit cards.
  - Built `risk_score` and `proxy_credit_group`.
  - **Missing / implicit missing:**
    - EF2 contained more than 50% empty strings; we dropped EF2 from both years to avoid cutting the sample in half.
    - Blank C4A values were recoded to `"No credit card ownership"` after checking C2A.
  - **Semantic checks:**
    - Removed 3 respondents where EF2 == "Yes" but all EF3_a–EF3_h == "No" (semantically impossible).
    - Verified that people without credit cards did not report unpaid-balance behavior in C4A.
  - Saved cleaned file as `shed24_cleaned2.csv`.

- **SHED 2020**
  - Similar variable selection with numeric coding (0/1, 0–3, -1).
  - Recomputed `risk_score` and `proxy_credit_group`.
  - Handled missing and refusal codes:
    - Used -2 as a sentinel for true missing in C4A.
    - Dropped rows with any -1 (refusal) in EF/C variables.
  - Standardized types: converted all binary variables and C4A to integer types.
  - Dropped EF2 to keep the 2020 and 2024 feature sets aligned.
  - Removed 137 rows in total; cleaned data saved as `shed20_cleaned2.csv`.

- **CFPB**
  - Checked that there were no missing values.
  - Verified that:
    - `date` values follow YYYY-MM format,
    - `credit_score_group` contains only the five expected categories,
    - `vol` and `vol_unadj` never take negative values.
  - Saved the validated dataset as `cfpd_cleaned2.csv`.

All cleaning logic, recoding decisions, and quality checks are documented in the **Data Quality** section and implemented in the scripts called by Snakemake.

---

## 6. Analyze

Analysis was performed in Python using the cleaned and integrated datasets:

- **SHED-only analysis**
  - Computed and plotted:
    - Emergency savings rates (EF1) in 2020 vs 2024.
    - “Good” emergency responses (EF3_A, EF3_C) vs “stressful” responses (EF3_B, EF3_D/E/F/G/H).
    - Credit card ownership (C2A) and unpaid balance behavior (C4A) by year.
  - These plots showed that financial literacy and financial stress indicators changed very little between 2020 and 2024.

- **CFPB-only analysis**
  - Aggregated credit card volume by year and credit score group.
  - Visualized log-transformed credit card volume for 2020 vs 2024.
  - Found that credit volume increased for all groups, especially Prime and Super-prime borrowers.

- **Merged SHED + CFPB analysis**
  - Examined how emergency savings (EF1), emergency response patterns, and unpaid balance behavior align with credit volume across risk groups.
  - Compared patterns between 2020 and 2024 to answer:
    - whether the relationship between financial literacy and credit card debt changed after COVID-19,
    - whether high-literacy households experienced smaller increases in new credit card limits.
  - Results suggest that the **strength and direction of the relationship remained stable**: financially stronger groups always received more credit, in both years.

Most of these analyses are encoded in the scripts `shed_analysis.py`, `cfpb_analysis.py`, and `merge_analysis.py`, and can also be explored interactively in the provided notebooks.

---

## 7. Preserve & Share

To make the project reusable and reproducible:

- **Code & workflow preservation**
  - All notebooks, scripts, and the `Snakefile` are stored in the public GitHub repository:
    - <https://github.com/oliviawudi/IS477-WJ>
  - The repository is licensed under MIT to allow reuse.
  - The Snakemake workflow encodes the full pipeline from raw data to final figures.

- **Data preservation**
  - Large raw and processed data files are stored in a shared Box folder:
    - <https://uofi.box.com/s/ietmow5zmh9ewrgu13qb431de4x3euce>
  - The Box link is documented in the report and `Reproduce.md`.
  - `.gitignore` ensures that these large files are not committed to GitHub, respecting size limits and licensing expectations.

- **Reproducibility documentation**
  - `Reproduce.md` describes:
    - how to set up the Python environment (`requirements.txt`, `pip freeze`),
    - how to download SHED and CFPB data from the official websites,
    - how to run the Snakemake workflow (`snakemake --cores 1`),
    - where to find the resulting CSVs and plots.
  - The workflow DAG (`dag.png`) visually documents provenance by showing how each intermediate file is produced.

Together, these steps complete the data lifecycle from planning and acquisition to long-term sharing and reuse.
